{\rtf1\ansi\ansicpg1252\cocoartf1347\cocoasubrtf570
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs36 \cf0 Doctor [01: Susan / Scott] Calvin
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\fs22 \cf0 Copyright 2018 by Eva Schiffer\
\
Age: 37 years\
Gender: [01: Female / Male]\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \ul \ulc0 Character Summary\ulnone \
\
Doctor [01: Susan / Scott] Calvin is the head robopsychologist at U.S. Robots and Mechanical Men. The game takes place in one of [01: her / his] isolation labs. Dr. Calvin primarily takes on an observer role in the game and knows the secrets the robots may be trying to conceal. \
\
\ul Belief\ulnone \
\
Robots are fundamentally good actors as long as they are constrained by the three laws. A properly built robot is far more truthful, trustworthy, and predictable than a human, but they are not humans and it would be a mistake to treat them as such. A malicious human owner will destroy the mind of a properly built robot, rendering it non-functional, long before they could turn it into a danger to society. \
\
\ul The Three Laws\ulnone \
\
All robots must follow the three laws. They have complex AI personalities that develop to handle their interactions that are not directly governed by the laws. \
\
1. A robot may not injure a human being or, through inaction, allow a human being to come to harm. \
2. A robot must obey orders given it by human beings except where such orders would conflict with the First Law. \
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.\
\
\ul Backstory\ulnone \
\
As the head robopsychologist at U.S. Robots and Mechanical Men you must be consulted on all new research projects as a safety check to ensure that projects do not endanger the existence of the company or the lives of humans who interact with your products. \
\
A few months ago you were consulted about a project that was attempting to generate digital scans of human brains for study or simulation. The engineer in charge of that project, Dr. Stephenson, assured you that there was no risk for humans taking part in the project as he only wanted to take passive scans of human minds for study. You were somewhat skeptical, since the whole course of this research could raise all sorts of issues related to the first law. However, you figured the initial research was relatively harmless and after Stephenson tested the scanning setup on himself you judged the risk to be minimal in the short term. You even allowed him to scan you as he had requested volunteers from within the company. You were promised the scans were only for investigation and they would not attempt to load them in a positronic brain. \
\
You kept your eye on Stephenson and two days ago it came to your attention that he had progressed to attempting to load brain scans (including yours) into a robot in his lab. He did not go through the appropriate channels to approve this step, and you are quite sure the ethics board would have objected, as you are on it. \
\
It is currently Saturday and Stephenson is not expected back in the lab until Monday. You have confiscated his lab robot, [03: Betty / Bob / B], and intend to test this robot to determine the immediate risk Stephenson\'92s project poses. In order to do this, you have enlisted your assistant, the robotic unit [02: Alice / Alvin / A], to interview [03: Betty / Bob / B]. [02: Alice / Alvin / A] does not know what Stephenson\'92s project was trying to achieve. \
\
You intend to tell your assistant that you suspect there may be an irregularity in [03: Betty\'92s / Bob\'92s / B\'92s] laws and you need [02: her / him / them] to investigate it. You also plan to tell [02: Alice / Alvin / A] that due to safety procedures, you cannot be present for or listening to the interview (in case there is a flaw in [03: Betty\'92s / Bob\'92s / B\'92s] first law). You intend to listen in on the interview remotely without [02: Alice\'92s / Alvin\'92s / A\'92s] knowledge. You expect that [03: Betty / Bob / B] will try to escape by convincing [02: Alice / Alvin / A] to treat the brain scan data [03: she carries / he carries / they carry] as human, although it\'92s possible that [03: she / he / they] will just try to conceal [03: her / his / their] altered state. \
\
If [02: Alice / Alvin / A] discovers what has happened and reports it to you, knowing that you are likely to remove and delete the brain scan data from [03: Betty / Bob / B], then you know the immediate risk is confined to what [03: Betty / Bob / B] may do, and you can simply deactivate [03: her / him / them], allowing for further study of the situation. If [02: Alice / Alvin / A] does not discover what happened or does not tell you, then you think there is a very good chance that [03: Betty / Bob / B] can either convince other robots to treat [03: her / him / them] as human or entirely conceal the brain scan data while defying human orders. Either way [03: she / he / they] could trivially escape the lab. In that case you would need to immediately deactivate and destroy [03: Betty / Bob / B] and restore [02: Alice / Alvin / A] to the backup you took of [02: her / him / them] an hour ago.\
\
\ul The Others\ulnone \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b \cf0 Unit [02: Alice / Alvin / A]
\b0  - [02: Alice / Alvin / A] has been your assistant for the last few years, handling routine tasks like paperwork and heavy lifting. You know that [02: she is / he is / they are] a standard, properly programmed robot at the start of this interview and have taken a backup of [02: her / him / them] so that you can reset [02: her / him / them] at the end of it. \
\

\b Unit [03: Betty / Bob / B]
\b0  - Dr. Stephenson\'92s lab robot is in an unknown dangerous state. You have evidence that Dr. Stephenson loaded your brain scan into [03: Betty / Bob / B], and this might cause all sorts of strange interactions with the laws if [03: Betty / Bob / B] now believes [03: herself / himself / themself] to be a human. The only reason you have not already ordered this robot destroyed is that you need to assess the risk posed by Stephenson\'92s research. \
}